{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/aayu-7/MountainCarGame-Using-ReinforcementLearning/blob/main/MountainCarGame.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1vZ74GdYS7gT"
   },
   "source": [
    " **Problem Description**\n",
    "\n",
    "In the MountainCar environment, a car starts at the bottom of a valley. The agent must learn to drive up a steep hill to reach the flag. Since the carâ€™s engine is not powerful enough to climb the hill directly, it must first build momentum by driving back and forth.\n",
    "The car's engine isn't powerful enough to drive directly up the hill. Instead, the agent must learn to build momentum by first moving left to gain speed and then driving right to reach the goal.\n",
    "\n",
    "    State Space: Continuous values for position and velocity.\n",
    "    Action Space: Three discrete actions:\n",
    "        0: Push left\n",
    "        1: No push\n",
    "        2: Push right\n",
    "    Reward: The agent gets -1 for each time step until it reaches the goal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2OMhh8fUTc-G"
   },
   "source": [
    "Training:\n",
    "\n",
    "    The agent is trained over 5000 episodes.\n",
    "    Epsilon-greedy policy ensures exploration initially, which decays over time to favor exploitation.\n",
    "\n",
    "Testing:\n",
    "\n",
    "    The trained agent is evaluated and its performance recorded as a video.\n",
    "\n",
    "Video Display:\n",
    "\n",
    "    The RecordVideo wrapper saves the video, and it is displayed using HTML in Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "R1pDBWRnRcEy",
    "outputId": "77437ff5-d306-4edd-f316-601a3a025d18"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy' has no attribute 'bool8'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 65\u001b[0m\n\u001b[0;32m     62\u001b[0m     action \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(q_table[state])\n\u001b[0;32m     64\u001b[0m \u001b[38;5;66;03m# Perform action\u001b[39;00m\n\u001b[1;32m---> 65\u001b[0m next_state_raw, reward, done, _ \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m next_state \u001b[38;5;241m=\u001b[39m discretize_state(next_state_raw, n_bins)\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m# Update Q-value\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\as219\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gym\\wrappers\\time_limit.py:50\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[0;32m     40\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \n\u001b[0;32m     42\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     48\u001b[0m \n\u001b[0;32m     49\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 50\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[1;32mc:\\Users\\as219\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gym\\wrappers\\order_enforcing.py:37\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 37\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\as219\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gym\\wrappers\\env_checker.py:37\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchecked_step \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchecked_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43menv_step_passive_checker\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "File \u001b[1;32mc:\\Users\\as219\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:233\u001b[0m, in \u001b[0;36menv_step_passive_checker\u001b[1;34m(env, action)\u001b[0m\n\u001b[0;32m    230\u001b[0m obs, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m result\n\u001b[0;32m    232\u001b[0m \u001b[38;5;66;03m# np.bool is actual python bool not np boolean type, therefore bool_ or bool8\u001b[39;00m\n\u001b[1;32m--> 233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(terminated, (\u001b[38;5;28mbool\u001b[39m, \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbool8\u001b[49m)):\n\u001b[0;32m    234\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    235\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpects `terminated` signal to be a boolean, actual type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(terminated)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    236\u001b[0m     )\n\u001b[0;32m    237\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncated, (\u001b[38;5;28mbool\u001b[39m, np\u001b[38;5;241m.\u001b[39mbool8)):\n",
      "File \u001b[1;32mc:\\Users\\as219\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\numpy\\__init__.py:427\u001b[0m, in \u001b[0;36m__getattr__\u001b[1;34m(attr)\u001b[0m\n\u001b[0;32m    424\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchar\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mchar\u001b[39;00m\n\u001b[0;32m    425\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m char\u001b[38;5;241m.\u001b[39mchararray\n\u001b[1;32m--> 427\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule \u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m has no attribute \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    428\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;18m__name__\u001b[39m, attr))\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'numpy' has no attribute 'bool8'"
     ]
    }
   ],
   "source": [
    "# Install the required library\n",
    "!\n",
    "\n",
    "# Import required libraries\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML\n",
    "import glob\n",
    "import io\n",
    "from base64 import b64encode\n",
    "from gym.wrappers import RecordVideo\n",
    "\n",
    "# Function to discretize state space\n",
    "def discretize_state(state, bins):\n",
    "    position_bins = np.linspace(-1.2, 0.6, bins)  # Discretize position\n",
    "    velocity_bins = np.linspace(-0.07, 0.07, bins)  # Discretize velocity\n",
    "    position_idx = np.digitize(state[0], position_bins) - 1\n",
    "    velocity_idx = np.digitize(state[1], velocity_bins) - 1\n",
    "    return (position_idx, velocity_idx)\n",
    "\n",
    "# Function to display video\n",
    "def show_video():\n",
    "    video_path = glob.glob('./video/*.mp4')[0]\n",
    "    video = io.open(video_path, 'r+b').read()\n",
    "    encoded = b64encode(video)\n",
    "    return HTML(data=f'''\n",
    "        <video width=\"640\" height=\"480\" controls>\n",
    "            <source src=\"data:video/mp4;base64,{encoded.decode('ascii')}\" type=\"video/mp4\">\n",
    "        </video>''')\n",
    "\n",
    "# Initialize the environment\n",
    "env = gym.make(\"MountainCar-v0\")\n",
    "\n",
    "# Hyperparameters\n",
    "n_bins = 20  # Number of bins for discretization\n",
    "alpha = 0.1  # Learning rate\n",
    "gamma = 0.99  # Discount factor\n",
    "epsilon = 1.0  # Exploration rate\n",
    "epsilon_decay = 0.995\n",
    "epsilon_min = 0.01\n",
    "n_episodes = 5000\n",
    "\n",
    "# Initialize Q-table\n",
    "n_actions = env.action_space.n\n",
    "q_table = np.zeros((n_bins, n_bins, n_actions))\n",
    "\n",
    "# Training loop\n",
    "# Training loop\n",
    "for episode in range(n_episodes):\n",
    "    # Unpack the tuple returned by env.reset()\n",
    "    state_raw, _ = env.reset()\n",
    "    state = discretize_state(state_raw, n_bins)\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        # Choose action: exploration vs exploitation\n",
    "        if np.random.random() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = np.argmax(q_table[state])\n",
    "\n",
    "        # Perform action\n",
    "        next_state_raw, reward, done, _ = env.step(action)\n",
    "        next_state = discretize_state(next_state_raw, n_bins)\n",
    "\n",
    "        # Update Q-value\n",
    "        best_next_action = np.argmax(q_table[next_state])\n",
    "        td_target = reward + gamma * q_table[next_state][best_next_action]\n",
    "        q_table[state][action] += alpha * (td_target - q_table[state][action])\n",
    "\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "    # Decay epsilon\n",
    "    if epsilon > epsilon_min:\n",
    "        epsilon *= epsilon_decay\n",
    "\n",
    "    if episode % 100 == 0:\n",
    "        print(f\"Episode {episode}, Total Reward: {total_reward}\")\n",
    "\n",
    "print(\"Training Complete!\")\n",
    "\n",
    "# Wrap environment for recording\n",
    "env = RecordVideo(env, \"./video\", episode_trigger=lambda x: True)\n",
    "\n",
    "# Test the trained agent\n",
    "state_raw, _ = env.reset()  # Unpack tuple during testing as well\n",
    "state = discretize_state(state_raw, n_bins)\n",
    "done = False\n",
    "while not done:\n",
    "    action = np.argmax(q_table[state])\n",
    "    next_state_raw, _, done, _ = env.step(action)\n",
    "    state = discretize_state(next_state_raw, n_bins)\n",
    "\n",
    "env.close()\n",
    "\n",
    "# Display the video\n",
    "show_video()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eRMVFBNFSMN7"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPvGfGVIgDiP4Nz+Mkxrqtm",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
